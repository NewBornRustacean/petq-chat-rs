use async_openai::{
    config::OpenAIConfig,
    types::{ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs},
    Client as OpenaiClient,
};
use axum::{
    extract::{Path, Query, State},
    response::sse::Event,
    response::{IntoResponse, Sse},
};
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::error::Error;
use std::sync::Arc;
use tokio::sync::Mutex;
use tokio_stream::wrappers::ReceiverStream;
use utoipa::{IntoParams, ToSchema};
use uuid::Uuid;

use crate::memory::SlidingWindow;

#[derive(Debug, Clone)]
pub struct PetProfile {
    pub name: String,
    pub species: String,
    pub age: String,
    pub health_concerns: String,
}

// Simulated in-memory "database" with user_id as key and PetProfile as value
pub type PetProfileDB = Arc<Mutex<HashMap<Uuid, PetProfile>>>;


#[derive(Deserialize, ToSchema, IntoParams)]
pub struct ChatParams {
    prompt: String,
}

#[derive(Serialize, Deserialize, Debug, ToSchema)]
pub struct ChatRecord {
    userid: Uuid,
    chatid: Uuid,
    window: SlidingWindow,
}

#[derive(Clone)]
pub struct SharedState {
    pub chat_collection: Arc<Mutex<HashMap<String, ChatRecord>>>,
    pub openai_client: Arc<OpenaiClient<OpenAIConfig>>,
    pub pet_profile_db: Arc<Mutex<HashMap<Uuid, PetProfile>>>,
}

/// Handles the chat streaming process, streaming chat completions generated by the OpenAI API.
///
/// This function receives a user ID and chat ID via the URL path and a prompt via query parameters.
/// It then spawns a new asynchronous task to generate chat completions using the OpenAI API,
/// sending the results as a server-sent event (SSE) stream to the client.
///
/// # Arguments
///
/// * `Path((userid, chatid))`: A tuple containing the user's UUID and the chat's UUID.
/// * `Query(params)`: The query parameters containing the prompt to be sent to the OpenAI API.
/// * `State((chat_collection, openai_client))`: A tuple containing the shared state, which includes:
///   - `chat_collection`: An `Arc<Mutex<HashMap<String, ChatRecord>>>` that stores chat records.
///   - `openai_client`: An `Arc<OpenaiClient<OpenAIConfig>>` used to interact with the OpenAI API.
///
/// # Returns
///
/// Returns an `Sse` response, which streams the chat completion results back to the client.
///
/// # Errors
///
/// This function returns a 500 Internal Server Error response if:
/// - There is an issue with sending the generated content through the channel.
/// - The OpenAI API returns an error.
///
#[utoipa::path(
    get,
    path = "/chat-stream/{userid}/{chatid}",
    params(
        ("userid" = Uuid, Path, description = "User ID"),
        ("chatid" = Uuid, Path, description = "Chat ID"),
        ChatParams
    ),
    responses(
        (status = 200, description = "Successful chat stream", body = String),
        (status = 500, description = "Internal server error"),
    )
)]
pub async fn chat_stream_handler(
    Path((userid, chatid)): Path<(Uuid, Uuid)>,
    Query(params): Query<ChatParams>,
    State(state): State<SharedState>,
) -> impl IntoResponse {
    let (tx, rx) = tokio::sync::mpsc::channel(100);

    let chat_collection_clone = Arc::clone(&state.chat_collection);
    let openai_client_clone = Arc::clone(&state.openai_client);
    let pet_profile_db_clone = Arc::clone(&state.pet_profile_db);

    // Fetch the pet profile from the "database"
    let pet_profile = {
        let db = pet_profile_db_clone.lock().await;
        db.get(&userid).cloned() // Clone the profile to pass it into the async task
    };

    tokio::spawn(async move {
        if let Some(pet_profile) = pet_profile {
            // Pet profile found, proceed with streaming
            if let Err(e) = generate_chat_stream(
                params.prompt.as_str(),
                tx,
                chat_collection_clone,
                userid,
                chatid,
                pet_profile, // Pass the fetched pet profile here
                openai_client_clone,
            )
                .await
            {
                eprintln!("Failed to stream chat: {}", e);
            }
        } else {
            // Pet profile not found, send an error message through the SSE stream
            let _ = tx.send("error: Pet profile not found".to_string()).await;
        }
    });

    let stream = ReceiverStream::new(rx);
    Sse::new(stream.map(|msg| Ok::<_, std::convert::Infallible>(Event::default().data(msg))))
}


/// Generates a stream of chat completions using the OpenAI API and sends them via a channel.
///
/// This function sends the user's prompt to the OpenAI API and streams the generated content back
/// through the provided channel. It accumulates the content as it's received and stores the final
/// chat record in the shared chat collection.
///
/// # Arguments
///
/// * `prompt`: A string slice containing the user's prompt.
/// * `tx`: A `tokio::sync::mpsc::Sender<String>` used to send the generated content back to the handler.
/// * `chat_collection`: An `Arc<Mutex<HashMap<String, ChatRecord>>>` that stores chat records.
/// * `userid`: The UUID of the user initiating the chat.
/// * `chatid`: The UUID of the chat session.
/// * `openai_client`: An `Arc<OpenaiClient<OpenAIConfig>>` used to interact with the OpenAI API.
///
/// # Returns
///
/// Returns `Result<(), Box<dyn Error>>`:
/// - `Ok(())` on success.
/// - An error wrapped in `Box<dyn Error>` if there is an issue during the process.
///
/// # Errors
///
/// This function returns an error if:
/// - The OpenAI API fails to process the request.
/// - The content fails to send through the provided channel.
///
async fn generate_chat_stream(
    prompt: &str,
    tx: tokio::sync::mpsc::Sender<String>,
    chat_collection: Arc<Mutex<HashMap<String, ChatRecord>>>,
    userid: Uuid,
    chatid: Uuid,
    pet_profile: PetProfile,
    openai_client: Arc<OpenaiClient<OpenAIConfig>>,
) -> Result<(), Box<dyn Error>> {
    let mut accumulated_content = String::new();

    // Lock the chat collection and retrieve or create a chat record for the given chatid
    let mut chats = chat_collection.lock().await;

    // Retrieve the chat record for this chatid, or create it with a new sliding window
    let chat_record = chats.entry(chatid.to_string()).or_insert_with(|| ChatRecord {
        userid,
        chatid,
        window: SlidingWindow::new(5), // This only happens once when the chatid is first encountered
    });

    // Add the user's current prompt to the sliding window
    chat_record.window.add(format!("User: {}", prompt));

    // Get the sliding window content as conversation history
    let history = chat_record.window.to_string();

    // Format the pet profile string
    let pet_profile_str = format!(
        "Pet name: {}, Species: {}, Age: {}, Health concerns: {}",
        pet_profile.name, pet_profile.species, pet_profile.age, pet_profile.health_concerns
    );

    // Format the prompt with the given template
    let formatted_prompt = format!(
        "[instruction] 당신은 사용자에게 수의사처럼 수의학 정보를 알려줘야합니다. pet profile과 recent chat history를 바탕으로 user query에 응답하세요.\n\n\
        [pet profile] {pet_profile_str}\n\n\
        [recent chat history] {history}\n\n\
        [user query] {prompt}"
    );

    // Build the request with the sliding window history included in the prompt
    let request = CreateChatCompletionRequestArgs::default()
        .model("gpt-4o-mini")
        .max_tokens(512u32)
        .messages([ChatCompletionRequestUserMessageArgs::default()
            .content(formatted_prompt) // Include history
            .build()?
            .into()])
        .build()?;

    let mut stream = openai_client.chat().create_stream(request).await?;

    while let Some(result) = stream.next().await {
        match result {
            Ok(response) => {
                for chat_choice in response.choices.iter() {
                    if let Some(ref content) = chat_choice.delta.content {
                        accumulated_content.push_str(content);

                        if let Err(e) = tx.send(content.to_string()).await {
                            eprintln!("Failed to send content: {}", e);
                        }
                    }
                }
            }
            Err(err) => {
                if let Err(e) = tx.send(format!("error: {err}")).await {
                    eprintln!("Failed to send error message: {}", e);
                }
            }
        }
    }

    chat_record.window.add(format!("System: {}", accumulated_content));
    Ok(())
}
