use async_openai::{
    config::OpenAIConfig,
    types::{ChatCompletionRequestUserMessageArgs, CreateChatCompletionRequestArgs},
    Client as OpenaiClient,
};
use axum::{
    extract::{Path, Query, State},
    response::sse::Event,
    response::{IntoResponse, Sse},
};
use futures::StreamExt;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::error::Error;
use std::sync::Arc;
use tokio::sync::Mutex;
use tokio_stream::wrappers::ReceiverStream;
use utoipa::{IntoParams, ToSchema};
use uuid::Uuid;

use crate::memory::SlidingWindow;


#[derive(Deserialize, ToSchema, IntoParams)]
pub struct ChatParams {
    prompt: String,
}

#[derive(Serialize, Deserialize, Debug, ToSchema)]
pub struct ChatRecord {
    userid: Uuid,
    chatid: Uuid,
    window: SlidingWindow, // Use SlidingWindow instead of plain strings
}

type SharedState = (Arc<Mutex<HashMap<String, ChatRecord>>>, Arc<OpenaiClient<OpenAIConfig>>);

/// Handles the chat streaming process, streaming chat completions generated by the OpenAI API.
///
/// This function receives a user ID and chat ID via the URL path and a prompt via query parameters.
/// It then spawns a new asynchronous task to generate chat completions using the OpenAI API,
/// sending the results as a server-sent event (SSE) stream to the client.
///
/// # Arguments
///
/// * `Path((userid, chatid))`: A tuple containing the user's UUID and the chat's UUID.
/// * `Query(params)`: The query parameters containing the prompt to be sent to the OpenAI API.
/// * `State((chat_collection, openai_client))`: A tuple containing the shared state, which includes:
///   - `chat_collection`: An `Arc<Mutex<HashMap<String, ChatRecord>>>` that stores chat records.
///   - `openai_client`: An `Arc<OpenaiClient<OpenAIConfig>>` used to interact with the OpenAI API.
///
/// # Returns
///
/// Returns an `Sse` response, which streams the chat completion results back to the client.
///
/// # Errors
///
/// This function returns a 500 Internal Server Error response if:
/// - There is an issue with sending the generated content through the channel.
/// - The OpenAI API returns an error.
///
#[utoipa::path(
    get,
    path = "/chat-stream/{userid}/{chatid}",
    params(
        ("userid" = Uuid, Path, description = "User ID"),
        ("chatid" = Uuid, Path, description = "Chat ID"),
        ChatParams
    ),
    responses(
        (status = 200, description = "Successful chat stream", body = String),
        (status = 500, description = "Internal server error"),
    )
)]
pub async fn chat_stream_handler(
    Path((userid, chatid)): Path<(Uuid, Uuid)>,
    Query(params): Query<ChatParams>,
    State((chat_collection, openai_client)): State<SharedState>,
) -> impl IntoResponse {
    let (tx, rx) = tokio::sync::mpsc::channel(100);

    let chat_collection_clone = Arc::clone(&chat_collection);
    let openai_client_clone = Arc::clone(&openai_client);

    tokio::spawn(async move {
        if let Err(e) = generate_chat_stream(
            params.prompt.as_str(),
            tx,
            chat_collection_clone,
            userid,
            chatid,
            openai_client_clone,
        )
        .await
        {
            eprintln!("Failed to stream chat: {}", e);
        }
    });

    let stream = ReceiverStream::new(rx);
    Sse::new(stream.map(|msg| Ok::<_, std::convert::Infallible>(Event::default().data(msg))))
}


/// Generates a stream of chat completions using the OpenAI API and sends them via a channel.
///
/// This function sends the user's prompt to the OpenAI API and streams the generated content back
/// through the provided channel. It accumulates the content as it's received and stores the final
/// chat record in the shared chat collection.
///
/// # Arguments
///
/// * `prompt`: A string slice containing the user's prompt.
/// * `tx`: A `tokio::sync::mpsc::Sender<String>` used to send the generated content back to the handler.
/// * `chat_collection`: An `Arc<Mutex<HashMap<String, ChatRecord>>>` that stores chat records.
/// * `userid`: The UUID of the user initiating the chat.
/// * `chatid`: The UUID of the chat session.
/// * `openai_client`: An `Arc<OpenaiClient<OpenAIConfig>>` used to interact with the OpenAI API.
///
/// # Returns
///
/// Returns `Result<(), Box<dyn Error>>`:
/// - `Ok(())` on success.
/// - An error wrapped in `Box<dyn Error>` if there is an issue during the process.
///
/// # Errors
///
/// This function returns an error if:
/// - The OpenAI API fails to process the request.
/// - The content fails to send through the provided channel.
///
async fn generate_chat_stream(
    prompt: &str,
    tx: tokio::sync::mpsc::Sender<String>,
    chat_collection: Arc<Mutex<HashMap<String, ChatRecord>>>,
    userid: Uuid,
    chatid: Uuid,
    openai_client: Arc<OpenaiClient<OpenAIConfig>>,
) -> Result<(), Box<dyn Error>> {
    let mut accumulated_content = String::new();

    // Lock the chat collection and retrieve or create a chat record for the given chatid
    let mut chats = chat_collection.lock().await;

    // Retrieve the chat record for this chatid, or create it with a new sliding window
    let chat_record = chats.entry(chatid.to_string()).or_insert_with(|| ChatRecord {
        userid,
        chatid,
        window: SlidingWindow::new(5), // This only happens once when the chatid is first encountered
    });

    // Add the user's current prompt to the sliding window
    chat_record.window.add(format!("User: {}", prompt));

    // Get the sliding window content as conversation history
    let history = chat_record.window.to_string();

    // Build the request with the sliding window history included in the prompt
    let request = CreateChatCompletionRequestArgs::default()
        .model("gpt-4o-mini")
        .max_tokens(512u32)
        .messages([ChatCompletionRequestUserMessageArgs::default()
            .content(format!("Conversation history:\n{}\nUser: {}", history, prompt)) // Include history
            .build()?
            .into()])
        .build()?;

    let mut stream = openai_client.chat().create_stream(request).await?;

    while let Some(result) = stream.next().await {
        match result {
            Ok(response) => {
                for chat_choice in response.choices.iter() {
                    if let Some(ref content) = chat_choice.delta.content {
                        accumulated_content.push_str(content);

                        if let Err(e) = tx.send(content.to_string()).await {
                            eprintln!("Failed to send content: {}", e);
                        }
                    }
                }
            }
            Err(err) => {
                if let Err(e) = tx.send(format!("error: {err}")).await {
                    eprintln!("Failed to send error message: {}", e);
                }
            }
        }
    }

    println!("{:?}", accumulated_content);
    Ok(())
}
